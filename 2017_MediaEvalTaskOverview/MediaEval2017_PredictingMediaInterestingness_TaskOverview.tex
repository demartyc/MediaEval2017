%%%% Proceedings format for most of ACM conferences (with the exceptions listed below) and all ICPS volumes.
\documentclass[sigconf]{acmart-me}
%%%% As of March 2017, [siggraph] is no longer used. Please use sigconf (above) for SIGGRAPH conferences.

%%%% Proceedings format for SIGPLAN conferences 
% \documentclass[sigplan, anonymous, review]{acmart}

%%%% Proceedings format for SIGCHI conferences
% \documentclass[sigchi, review]{acmart}

%%%% To use the SIGCHI extended abstract template, please visit
% https://www.overleaf.com/read/zzzfqvkmrfzn

\usepackage{booktabs} % For formal tables
\usepackage{url}
\usepackage{color}
\usepackage{enumitem}
\hyphenation{Media-Eval}




% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{}

% ISBN
\acmISBN{}

%Conference
\acmConference[MediaEval'17]{Multimedia Evaluation Workshop}{13-15 September 2017}{Dublin, Ireland} 
\acmYear{}
\copyrightyear{}

\acmPrice{}


\begin{document}
\title{MediaEval 2017 Predicting Media Interestingness Task}

%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
 % \texttt{acmart.pdf} document}


%%If you want to use multi-column authors that's fine. Comment out the next lines, and then uncomment below.
%\author{Martha Larson\textsuperscript{1}, Gareth Jones\textsuperscript{2}, Bogdan Ionescu\textsuperscript{3},\\ Mohammad Soleymani\textsuperscript{4}, Guillaume Gravier\textsuperscript{5}}
%\affiliation{\textsuperscript{1}Radboud University, Netherlands\\ \textsuperscript{2}Dublin City University, Ireland \\ \textsuperscript{3}University Politehnica of Bucharest, Romania \\ \textsuperscript{4}University of Geneva, Switzerland \\ \textsuperscript{5}CNRS IRISA and Inria Rennes, France}
%\email{m.a.larson@tudelft.nl, gareth.jones@computing.dcu.ie, bionescu@imag.pub.ro}
%\email{mohammad.soleymani@unige.ch, guig@irisa.fr}
%
\author{Claire-H\'{e}l\`{e}ne Demarty\textsuperscript{1}, Mats Sj\"{o}berg\textsuperscript{2}, Bogdan Ionescu\textsuperscript{3}, Thanh-Toan Do\textsuperscript{4}, \\ Michael Gygli\textsuperscript{5}, Ngoc Q. K. Duong\textsuperscript{1}}
\affiliation{\textsuperscript{1}Technicolor, Rennes, France\\ 
\textsuperscript{2}Dept.\ of Computer Science and Helsinki Institute for Information
Technology HIIT, University of Helsinki, Finland\\ 
\textsuperscript{3}LAPI, University Politehnica of Bucharest, Romania\\
\textsuperscript{4}University of Science, Vietnam, University of Adelaide, Australia\\
\textsuperscript{5}ETH Zurich, Switzerland \& Gifs.com, US}
%\email{\small \{claire-helene.demarty, quang-khanh-ngoc.duong\}@technicolor.com, mats.sjoberg@helsinki.fi}
%\email{\small bionescu@imag.pub.ro, thanhtoan\_do@sutd.edu.sg, gygli@vision.ee.ethz.ch}



\renewcommand{\shortauthors}{C.H. Demarty et al.}


\begin{abstract}
In this paper, the Predicting Media Interestingness task which is running for the 
second year as part of the Media\-Eval 2017 Benchmarking Initiative for Multimedia Evaluation,
is presented. For the task, participants are expected to create systems that automatically
select images and video segments that are considered to be the most
interesting for a common viewer. All task characteristics are described, namely
the task use case and challenges, the released data set and ground truth, the required
participant runs and the evaluation metrics.
\end{abstract}


\maketitle

\vspace{-0.2cm}
\section{Introduction}
%===========================================

%\textcolor{red}{Bogdan, could you help for this?}

Predicting the interestingness of media content has been an active area of research in the computer vision community for several years now~\cite{Jiang2013, Gygli2013, Dhar2011, Amengual2015} and it has even been studied earlier in the psychological community~\cite{Berlyne1960, Smith1985, Silvia2006}. However, there were multiple competing definitions of interestingness, only a few publicly available datasets, and until last year, no public benchmark existed to assess the interestingness of content. In 2016, a task for the Prediction of Media Interestingness was proposed in the Media\-Eval 2016 Benchmarking Initiative for Multimedia Evaluation. This task was also an opportunity to propose a clear definition of interestingness, compatible with a real-world industry use case at Technicolor\footnote{\url{http://www.technicolor.com}}. The 2017 edition of the Media\-Eval benchmark includes a follow-up of the Predicting Media Interestingness Task.
%%
This paper gives an overview of the task description in its second year, together with a description of the data and ground truth. The required runs and chosen evaluation metrics are also detailed. In all cases, changes in this year's edition are highlighted compared to last year's edition.


\vspace{-0.2cm}
\section{Task description}
%================================================

The Predicting Media Interestingness Task was proposed for the first time last year. This year's edition is a follow-up which builds incrementally upon the previous experience. The task requires participants to automatically select images and/or video segments which are considered to be the most interesting for a common viewer. Interestingness of media is to be judged based on visual appearance, audio information and text accompanying the data, including movie metadata. To solve the task, participants are strongly encouraged to deploy multimodal approaches.

As in 2016, interestingness should be assessed according to a practical use case at Technicolor, which involves helping professionals to illustrate a Video on Demand (VOD) web site by selecting some interesting frames and/or video excerpts for the movies. The frames and excerpts should be suitable in terms of helping a user to make his/her decision about whether he/she is interested in watching the whole movie.
Once again, two subtasks will be offered to participants, which correspond to two types of available media content, namely images and videos. Participants are encouraged to submit to both subtasks. In both cases, the task will be considered as a binary classification \emph{and} a ranking task. Prediction will be carried out on a per movie basis.
% GYGLIM: Since the evaluation metric is MAP@10 the task is not really a classification, but a ranking task!
%Ok with Nogc we tried to modify this part to add both binary classification and regression => it is already released this way at least on the wiki, we cannot change the rules now. Please tell us if this is clearer.
The two taskes are:

%\begin{itemize}
%\item 
\emph{\bf Predicting Image Interestingness} Given a set of key-frames extracted from a certain movie, the task involves automatically (1) identifying those images that viewers report to be interesting and (2) ranking all images according to their level of interestingness. To solve the task, participants can make use of visual content as well as accompanying metadata, e.g., Internet data about the movie, social media information, etc.

%\item 
\emph{\bf Predicting Video Interestingness} Given a set of video segments extracted from a certain movie, the task involves automatically (1) identifying the segments that viewers report to be interesting and (2) ranking all segments according to their level of interestingness. To solve the task, participants can make use of visual and audio data as well as accompanying metadata, e.g., subtitles, Internet data about the movie, etc.
%\end{itemize}

\vspace{-0.2cm}
\section{Data description}
%==============

The data is extracted from Creative Commons licensed Hollywood-like videos: 103 movie trailers and 4 continuous extracts of ca. 15min from full-length movies. 
%%
For the video interestingness subtask, the data consists of video segments obtained after a manual segmentation. These segments correspond to shots (video shots are the continuous frame sequences recorded between a camera turn on and off) for all videos but four. Their average duration is of one second. The four last videos, which correspond to the full-length movie extracts cited above, were manually segmented into longer segments (243) with an average duration of 11.4s, to better take into account a certain unity of meaning and the audio information of the resulting segments. For the image subtask, the data consists of collections of key-frames extracted from the video segments used for the video subtask (one key-frame per segment). This will allow comparing results from both subtasks. The extracted key-frame corresponds to the frame in the middle of each video segment.
In total, 7,396 video segments and 7,396 key-frames are released in the development set, whereas the test set consists of 2435 video segments and the same number of key-frames.

%\textcolor{red}{*** Here we could just copy-past what was said for the provided features last year or rewrite everything. Should we talk about the bug that was discovered in the face-tracking feature and the advice not to take into account negative values ? I am not sure of this. I am copying text from last year below. If we agree that this needs rephrasing, Michael, can you help for this part?***}
% GYGLIM: Not sure what exactly you mean here? The features description? I think it's ok, except we could simplify it by removing some details like the follwing:
% MFCC (Mel-Frequency Cepstral Coefficients) computed over 32ms time-windows with 50% overlap. The cepstral vectors are concatenated with their first and second derivatives
% --> maybe remmve the second sentence
% Ok I am not changing anything for the moment. This is copy paste from last year. 

To facilitate participation from various communities, we also
provide some pre-computed content descriptors, namely: \emph{low
level features} --- \emph{dense SIFT} (Scale Invariant Feature
Transform) which are computed following the original work
in~\cite{Lowe2004}, except that the local frame patches are densely
sampled instead of using interest point detectors. A codebook of 300
codewords is used in the quantization process with a spatial pyramid
of three layers~\cite{Lazebnik2006}; \emph{HoG descriptors}
(Histograms of Oriented Gradients)~\cite{Dalal2005} are computed
over densely sampled patches. Following~\cite{Xiao2010}, HoG
descriptors in a $2\times2$ neighborhood are concatenated to form a
descriptor of higher dimension; \emph{LBP} (Local Binary
Patterns)~\cite{Ojala2002}; \emph{GIST} are computed based on the
output energy of several Gabor-like filters (8 orientations and 4
scales) over a dense frame grid like in~\cite{Oliva2001};
\emph{color histogram} computed in the HSV space
(Hue-Saturation-Value); \emph{MFCC} (Mel-Frequency Cepstral
Coefficients) computed over 32ms time-windows with 50\%
overlap. The cepstral vectors are concatenated with their first and
second derivatives; \emph{fc7 layer} (4,096 dimensions) and
\emph{prob layer} (1,000 dimensions) of AlexNet~\cite{Jiang2015};
\emph{mid level face detection and tracking related
features}\footnote{\url{http://multimediaeval.org/mediaeval2016/persondiscovery/}}
--- obtained by face tracking-by-detection in each video shot with a
HoG detector~\cite{Dalal2005} and the correlation tracker proposed
in~\cite{Danelljan2014}. In addition to these frame-based features, we provide C3D~\cite{tran2015learning} features, which were extracted from \emph{fc6 layer} (4,096 dimensions) and averaged on a segment level.


\vspace{-0.2cm}
\section{Ground truth}
%============

Both video and image data was manually and independently annotated in terms of interestingness by human assessors, to allow to study the correlation between the two subtasks.
%%
A dedicated web-based annotation tool was developed by the organising team for the previous edition of the task~\cite{demarty2016mediaeval}. 
%%
This year some incremental improvements were added, and the tool was released as free and open source software\footnote{\url{https://github.com/mvsjober/pair-annotate}}.
%, so that others can benefit from it and contribute
%improvements\footnote{\url{https://github.com/mvsjober/pair-annotate}}.
%%
Overall, more than 202 annotators participated in the annotation for the video data and 144 for the images.
%%
The cultural distribution is over 20 different countries in the world.

%% FIXME numbers may change still...
%%
%% 2017, so far...
%% Video side: 202
%% Image side: 144
%% 20 countries

As in last year's setup we use a pair-wise comparison protocol~\cite{Bradley} where annotators are provided with a pair of images/shots at a time and asked to tag which one in the pair is the more interesting for them.
%%
As a change from last year, we now ask the question in a way more directly connected to the commercial application: ``Which image/video makes you more interested in watching the whole movie?'', with the intent to make the annotators' decision clearer.
%%
%We felt this would make the annotators' decision clearer, as otherwise ``interestingness'' can be interpreted in many ways.
As an exhaustive annotation of all possible pairs is practically impossible due to the required human resources, a boosting selection was used instead.
%%
In particular, we used a modified version of the adaptive square design method~\cite{Li-SPIE2013}, in which several annotators participated in each iteration.  
%%
In this method the number of comparisons for each iteration is reduced from all possible pairs $n(n-1)/2 \sim \mathcal{O}(n^2)$ to a subset of pairs $n(\sqrt{n}-1) \sim \mathcal{O}(n^{\frac{3}{2}})$, where $n$ is the number of segments or images.
%%
For the development set, we started from iteration 6, as we could reuse the annotations done last year.
%%
To achieve the ranking used as the basis for the next round, the pair-based annotations are aggregated with the Bradley-Terry-Luce (BTL) model computation~\cite{Bradley} resulting in an interestingness degree for each image/shot.
%%
Previously the same procedure was also used to get the final interestingness values.
%%
This year we used an alternative method, which took into account all pair comparisons from all rounds done this year into a single large BTL calculation.
%%
This was done mainly because we discovered afterwards that some annotations from earlier rounds had to be discarded, because of some cheating annotators. %%
These annotators occasionally switched to cheating, where they simply always selected the first, or the second item as the most interesting one without actually assessing the media contents.
%%
In the development set as much as 10\% of the annotations were marked as invalid and not included in the final BTL calculation. We added some heuristic anti-cheating measures to the system, although it is not possible to perfectly detect all cheating.
%%
Unfortunately, in the iterative approach, we could only discard annotations from the most recent round, as it would be based on the previous round's BTL output, which is why we developed another solution to compute the final BTL ranking.
%%
The final binary decisions are obtained using a thresholding scheme that tries to detect the boundary where interestingness values make the ``jump'' between the underlying distributions of the non interesting and interesting populations.  See last year's overview paper for a more detailed description~\cite{demarty2016mediaeval}.

% Otherwise, i think it is exactly the same protocol as last year, except that we compute the final BTL on all the new data with an initialization from last year ranking. 1/ This was decided because of the cheating process to help remove the false data 2/ We checked visually that the rankings were ok and even a little better for some of the videos compared to what was obtained in 2016 (but very similar to what would have been obtained this year by following the same iterative process to compute the BTL) => this validates both ways of computing the BTL (one against the other) + the fact that the increased number of iterations did improve the ranking at least for the images.
  
% Should we talk about the anti-cheating measures?

%A new issue this year was that we discovered that some annotators were cheating.
%%

%%

%, for example if the same position has been selected repeatedly for a given number of times in a given period of time that user would be logged out and presented with a notice about cheating having been detected.
%%
%Votes matching these criteria were also marked as ``invalid'' and not included in the final BTL calculation.
%%
%%
%The high number can be explained by the fact that a cheater can annotate much faster than an honest annotator.
%%
%Detecting cheating is ultimately a very difficult task, as malicious annotators may easily create a new pattern in response to anti-cheating measures.
%%
%Furthermore, it is not possible to unambiguously detect cheat votes by just analysing the voting patterns.  For example, an honest annotator may indeed get a surprisingly long sequence of votes for the same position just by chance, or a cheater may develop a method to select the items randomly.
%%
%A possible idea for future versions of the system would be to occasionally present pairs for which the system already has a lot of consistent evidence for to test the annotator's honesty.
%%
%If the annotator fails these several times it might be a good indication that he or she is cheating.
%% 
%On the other hand, it is possible that annotators cheat only on very difficult cases.  In fact the difficulty of many of the judgements might be the reason why some people grow tired of annotating honestly.
%\textcolor{red}{GYGLIM: This tells the story of events in the annotation process, rather than being useful for getting an overview. It is too long and needs serious rewriting.
%Also, a better check, in my opinion is to show the same pair again at a later time (and switched) and see if they still perfer the same one.}

% CHD: nope, this does not work. We experienced that some people may change their mind (and some even claimed that they did so) during the annotations, in case of difficult choices.
% Plus add a few figures as last year about the annotators population.

\vspace{-0.2cm}
\section{Run description}
%==============

Every team can submit up to 10 runs, 5 per subtask. For each subtask, a required run is defined: \emph{Image subtask - required run:} classification is to be achieved with the use of the visual information. External data is allowed. \emph{Video subtask - required run:} classification is to be achieved with the use of \emph{both} audio and visual information. External data is allowed.
Apart from these required runs, any additional run for each subtask will be considered as a general run, i.e., anything is allowed, both from the method point of view and the information sources. 

%\textcolor{red}{GYGLIM: I wasn't aware we were allowing this many runs. I think it's too much and total of 3-5 would do}
%CHD: well this is too late, as it is published so on the wiki page. It came from a long discussion between us: 5 runs for the 2 tasks altogether was not fair, which is why we decided to have the same nb per subtask. Going up to 5 was a decision we took because the evaluation of so many runs is simple. Let's see how it goes this year, we could then decide for less runs next year.

\vspace{-0.2cm}
\section{Evaluation}
%===========

For both subtasks, the official evaluation metric will be the mean average precision at 10 (MAP@10) computed over all videos, and over the top 10 best ranked images/video shots. MAP@10 is selected because it reflects the VOD use case, where the goal is to select a small set of the most interesting images or video segments for each movie. To provide a large overview of the systems' performances, other common metrics will also be provided. All metrics will be computed by using the \texttt{trec\_eval} tool from
NIST\footnote{\url{http://trec.nist.gov/trec\_eval/}}.

\vspace{-0.2cm}
\section{Conclusions}
%===========
In the 2017 Predicting Media Interestingness task a complete and comparative framework for the evaluation of content interestingness is proposed. 
Details on the methods and results of each individual
participant team can be found in the working note papers of the
MediaEval 2017 workshop proceedings.

\vspace{-0.2cm}
\begin{acks}
%========================
{\small
We would like to thank Yu-Gang Jiang and Baohan Xu
from the Fudan University, China, Herv\'{e} Bredin, from LIMSI,
France, and Michael Gygli for providing the features that accompany the
released data. Part of the task was funded under research grant PN-III-P2-2.1-PED-2016-1065, agreement 30PED/2017, project SPOTTER.}
\end{acks}


\bibliographystyle{ACM-Reference-Format}
\def\bibfont{\small} % comment this line for a smaller fontsize
\bibliography{MediaEval2017_PredictingMediaInterestingness_TaskOverview}

\end{document}
